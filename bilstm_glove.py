# -*- coding: utf-8 -*-
"""bilstm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cjdXxG4jCwdqVAl9smOelbqrGPJciK10
"""

import pandas as pd
from xgboost import XGBClassifier
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier 
from sklearn.linear_model import LogisticRegression 
from sklearn import svm
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc,precision_recall_fscore_support, f1_score,roc_auc_score,accuracy_score, classification_report
from sklearn.feature_selection import SelectKBest, chi2, VarianceThreshold , f_classif
from sklearn.model_selection import train_test_split , cross_val_predict,cross_val_score,ShuffleSplit
from sklearn import model_selection
from sklearn import svm
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords

from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten
from keras.layers import GlobalMaxPooling1D,Bidirectional
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer

from google.colab import drive
drive.mount('/content/drive')

data=pd.read_csv("/content/drive/MyDrive/ML Assignment/Assignment_6/Preprocessed_Final_Datatset_new.csv",encoding='latin1')

def read_glove_vecs(glove_file):
    with open(glove_file, 'r', encoding="utf8") as f:
        words = set()
        word_to_vec_map = {}
        for line in f:
            line = line.strip().split()
            curr_word = line[0]
            words.add(curr_word)
            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)
        
        i = 1
        words_to_index = {}
        index_to_words = {}
        for w in sorted(words):
            words_to_index[w] = i
            index_to_words[i] = w
            i = i + 1
    return words_to_index, index_to_words, word_to_vec_map

word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/drive/MyDrive/ML Assignment/Assignment_6/glove.6B.50d.txt')

cleaned_tokens_list=[]
b = data['Clean_TweetText'].values.astype('U')
a=b.ravel()
# creating the dataframe 
df_cleantweet = pd.DataFrame(data = a)
df_labels = pd.DataFrame(data=data['label'])
print(df_cleantweet)

for index,row in df_cleantweet.iterrows():
    first = df_cleantweet[0][index]
    second = df_labels['label'][index]
    cleaned_tokens_list.append((first,second))

from time import time
unks = []
UNKS = []

# This function will act as a "last resort" in order to try and find the word
# in the words embedding layer. It will basically eliminate contiguously occuring
# instances of a similar character
def cleared(word):
    res = ""
    prev = None
    for char in word:
        if char == prev: continue
        prev = char
        res += char
    return res


def sentence_to_indices(sentence_words, word_to_index, max_len, i):
    global X, Y
    sentence_indices = []
    for j, w in enumerate(sentence_words):
        try:
            index = word_to_index[w]
        except:
            UNKS.append(w)
            w = cleared(w)
            try:
                index = word_to_index[w]
            except:
                index = word_to_index['unk']
                unks.append(w)
        X[i, j] = index

# Here we will utilize the already computed 'cleaned_tokens_list' variable
   

start_time = time()

list_len = [len(i) for i, j in cleaned_tokens_list]
max_len = max(list_len)
print('max_len:', max_len)

X = np.zeros((len(cleaned_tokens_list), max_len))
Y = np.zeros((len(cleaned_tokens_list), ))

for i, tk_lb in enumerate(cleaned_tokens_list):
    tokens, label = tk_lb
    sentence_to_indices(tokens, word_to_index, max_len, i)
    Y[i] = label
    
print('Data Prepared for model, CPU Time:', time() - start_time)


print(X[:5])
print(Y[:5])

def pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len):
    vocab_len = len(word_to_index) + 1
    emb_dim = word_to_vec_map["unk"].shape[0] #50
    
    emb_matrix = np.zeros((vocab_len, emb_dim))
    
    for word, idx in word_to_index.items():
        emb_matrix[idx, :] = word_to_vec_map[word]
        
    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False, input_shape=(max_len,))
    embedding_layer.build((None,))
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer

from keras.layers import Dense, LSTM
model = Sequential()

model.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))
model.add(Bidirectional(LSTM(units=100, return_sequences=True)))
model.add(Bidirectional(LSTM(units=100, return_sequences=False)))
model.add(Dense(units=1, activation='sigmoid'))

model.summary()

from keras.layers.convolutional import Conv1D,Conv2D
from keras.layers import Dense, Flatten, Conv2D, MaxPooling1D,ZeroPadding2D,BatchNormalization
from keras.layers import Dense, LSTM
from keras.layers import Dense, LSTM
from keras.optimizers import Adam,RMSprop
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)
print("here")
model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 1, batch_size = 1000, verbose=1)

fpred=model.predict_classes(X_test)
print(fpred)

import pickle
pickle_out=open("prediction_files\\y_pred_bilstm_glove.p","wb")
pickle.dump(fpred,pickle_out)
pickle_out.close()

pickle_out=open("prediction_files\\Y_test_bilstm_glove_ACTUAL.p","wb")
pickle.dump(Y_test,pickle_out)
pickle_out.close()



predictions=fpred
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report

accuracy = accuracy_score(Y_test, fpred)

print("Accuracy : %.2f%%" % (accuracy * 100.0))

print(precision_recall_fscore_support(Y_test, predictions, average=None,labels=[0,1]))
print("Average Precision : ",precision_score(Y_test, predictions, average='weighted'))
print("Recall weighted : ",recall_score(Y_test, predictions, average='weighted'))
print("Roc_auc score : ",roc_auc_score(Y_test, predictions,average='weighted'))

f1 = f1_score(Y_test, predictions, average='macro')
print("FI Score" , f1)

print("Testing Classification report")
print(classification_report(Y_test, predictions, target_names=['0','1']))


print("\n")

import warnings
warnings.filterwarnings("ignore")